{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSEMN Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Create a random list of number and then save it to a text file named \"simple_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "npr.seed(42)\n",
    "with open(\"simple_data2.txt\",\"+a\") as f:\n",
    "    a=npr.random((100,1))\n",
    "    for i in a:\n",
    "        f.write(str(i))\n",
    "\n",
    "df=pd.DataFrame(a)\n",
    "df.to_csv(\"simple_data.txt\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Create a random matrix of 5x5 and then save it to a text file named \"data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03142919 0.63641041 0.31435598 0.50857069 0.90756647]\n",
      " [0.24929223 0.41038292 0.75555114 0.22879817 0.07697991]\n",
      " [0.28975145 0.16122129 0.92969765 0.80812038 0.63340376]\n",
      " [0.87146059 0.80367208 0.18657006 0.892559   0.53934224]\n",
      " [0.80744016 0.8960913  0.31800347 0.11005192 0.22793516]]\n"
     ]
    }
   ],
   "source": [
    "m=npr.random((5,5))\n",
    "print(m)\n",
    "df=pd.DataFrame(m)\n",
    "df.to_csv(\"data.txt\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Load the saved txt file of point 2 and convert it to a csv file (by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data.txt','r') as p:\n",
    "    with open('data.csv','w') as n:\n",
    "        writer=csv.writer(n)\n",
    "        for row in p:\n",
    "            writer.writerow(row.split())\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. load the binary file named *credit_card.dat* and convert the data into the real credit-card number.\n",
    "Each line correspond to a credit card number.\n",
    "Each character is composed by 6 bit (even the space) and the last 4 bit are just a padding\n",
    "\n",
    "**hint**: use the `chr()` function to convert a number to a char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7', '6', '4', '8', ' ', '5', '6', '7', '3', ' ', '3', '7', '7', '5', ' ', '2', '2', '7', '1']\n",
      "['3', '2', '5', '7', ' ', '8', '2', '4', '7', ' ', '3', '3', '5', '4', ' ', '2', '2', '6', '6']\n",
      "['2', '7', '2', '2', ' ', '0', '0', '0', '1', ' ', '4', '0', '1', '1', ' ', '6', '6', '5', '2']\n",
      "['0', '6', '6', '1', ' ', '3', '0', '6', '3', ' ', '3', '7', '4', '2', ' ', '3', '1', '5', '0']\n",
      "['0', '4', '3', '2', ' ', '1', '6', '0', '8', ' ', '1', '4', '6', '2', ' ', '4', '7', '4', '2']\n",
      "['5', '8', '2', '7', ' ', '2', '0', '2', '7', ' ', '8', '7', '8', '5', ' ', '7', '3', '0', '3']\n",
      "['5', '7', '7', '4', ' ', '8', '5', '2', '8', ' ', '2', '0', '8', '7', ' ', '1', '1', '1', '7']\n",
      "['8', '1', '4', '0', ' ', '1', '2', '1', '0', ' ', '6', '3', '5', '2', ' ', '2', '8', '4', '5']\n",
      "['5', '7', '6', '4', ' ', '1', '1', '3', '3', ' ', '7', '3', '0', '1', ' ', '7', '1', '0', '0']\n",
      "['6', '4', '5', '6', ' ', '1', '7', '3', '7', ' ', '4', '1', '2', '6', ' ', '6', '7', '2', '6']\n",
      "['1', '2', '2', '8', ' ', '8', '6', '3', '1', ' ', '7', '3', '8', '2', ' ', '0', '0', '0', '0']\n",
      "['7', '0', '5', '1', ' ', '0', '1', '6', '0', ' ', '5', '3', '7', '4', ' ', '3', '1', '6', '6']\n",
      "['0', '6', '1', '8', ' ', '3', '5', '8', '7', ' ', '1', '6', '3', '0', ' ', '6', '3', '7', '6']\n",
      "['1', '5', '4', '5', ' ', '5', '4', '5', '4', ' ', '7', '4', '4', '4', ' ', '5', '6', '3', '6']\n",
      "['6', '7', '3', '5', ' ', '3', '1', '1', '6', ' ', '3', '2', '0', '2', ' ', '6', '8', '3', '4']\n",
      "['7', '2', '8', '7', ' ', '5', '0', '1', '1', ' ', '1', '5', '4', '7', ' ', '8', '4', '1', '3']\n",
      "['7', '0', '3', '3', ' ', '2', '6', '0', '7', ' ', '3', '3', '2', '8', ' ', '4', '2', '0', '0']\n",
      "['2', '5', '6', '8', ' ', '5', '2', '4', '4', ' ', '1', '8', '7', '4', ' ', '5', '0', '2', '4']\n",
      "['1', '6', '8', '4', ' ', '2', '2', '5', '3', ' ', '7', '5', '7', '0', ' ', '7', '1', '1', '8']\n",
      "['0', '6', '7', '2', ' ', '2', '5', '7', '6', ' ', '0', '5', '7', '5', ' ', '6', '6', '3', '1']\n",
      "['6', '3', '3', '2', ' ', '8', '3', '5', '3', ' ', '8', '7', '8', '7', ' ', '1', '3', '4', '0']\n",
      "['1', '8', '1', '3', ' ', '3', '3', '6', '1', ' ', '1', '1', '7', '5', ' ', '4', '2', '1', '1']\n",
      "['2', '4', '7', '7', ' ', '6', '4', '5', '0', ' ', '8', '8', '4', '0', ' ', '2', '3', '6', '8']\n",
      "['5', '5', '1', '2', ' ', '3', '5', '0', '5', ' ', '2', '5', '6', '3', ' ', '1', '3', '2', '6']\n",
      "['3', '0', '8', '3', ' ', '7', '8', '8', '2', ' ', '0', '6', '2', '1', ' ', '0', '0', '2', '5']\n",
      "['4', '5', '2', '1', ' ', '5', '1', '4', '8', ' ', '8', '0', '4', '5', ' ', '0', '3', '3', '4']\n",
      "['7', '5', '6', '3', ' ', '3', '6', '5', '4', ' ', '8', '7', '1', '3', ' ', '5', '7', '8', '7']\n",
      "['8', '3', '2', '4', ' ', '2', '6', '6', '4', ' ', '0', '4', '7', '6', ' ', '5', '5', '6', '1']\n",
      "['0', '5', '6', '5', ' ', '2', '5', '0', '4', ' ', '7', '1', '6', '8', ' ', '3', '5', '1', '0']\n",
      "['5', '1', '0', '7', ' ', '5', '5', '0', '7', ' ', '1', '7', '6', '7', ' ', '0', '7', '3', '8']\n",
      "['2', '4', '6', '2', ' ', '1', '8', '2', '1', ' ', '2', '4', '4', '8', ' ', '1', '4', '4', '3']\n",
      "['2', '7', '8', '8', ' ', '0', '6', '3', '8', ' ', '6', '8', '6', '1', ' ', '6', '5', '5', '4']\n",
      "['5', '8', '5', '1', ' ', '5', '8', '7', '3', ' ', '5', '4', '7', '4', ' ', '0', '5', '4', '7']\n",
      "['0', '6', '7', '0', ' ', '1', '0', '0', '4', ' ', '4', '0', '1', '3', ' ', '2', '6', '5', '5']\n",
      "['5', '8', '7', '4', ' ', '5', '5', '0', '6', ' ', '3', '0', '4', '8', ' ', '0', '8', '0', '6']\n",
      "['2', '8', '0', '5', ' ', '5', '4', '0', '1', ' ', '8', '4', '6', '2', ' ', '1', '2', '6', '0']\n",
      "['5', '0', '8', '3', ' ', '8', '4', '0', '6', ' ', '6', '3', '1', '0', ' ', '1', '8', '6', '2']\n",
      "['1', '0', '7', '6', ' ', '1', '4', '4', '5', ' ', '3', '0', '1', '3', ' ', '2', '2', '6', '6']\n",
      "['8', '4', '4', '0', ' ', '4', '8', '0', '4', ' ', '4', '8', '4', '4', ' ', '5', '2', '7', '7']\n",
      "['4', '7', '5', '8', ' ', '6', '1', '4', '1', ' ', '0', '6', '8', '6', ' ', '1', '3', '8', '7']\n",
      "['7', '5', '8', '6', ' ', '0', '6', '7', '5', ' ', '0', '3', '1', '5', ' ', '2', '5', '6', '8']\n",
      "['2', '5', '4', '4', ' ', '1', '2', '5', '8', ' ', '7', '4', '3', '2', ' ', '5', '1', '6', '5']\n",
      "['3', '4', '7', '4', ' ', '5', '0', '2', '3', ' ', '4', '4', '3', '4', ' ', '5', '6', '2', '6']\n",
      "['1', '4', '1', '0', ' ', '0', '2', '7', '0', ' ', '0', '4', '3', '4', ' ', '5', '0', '8', '6']\n",
      "['7', '3', '1', '5', ' ', '4', '4', '4', '6', ' ', '1', '1', '0', '4', ' ', '4', '2', '1', '5']\n",
      "['0', '2', '2', '4', ' ', '7', '7', '4', '2', ' ', '8', '3', '0', '0', ' ', '0', '2', '6', '6']\n",
      "['0', '1', '7', '0', ' ', '2', '7', '0', '0', ' ', '3', '1', '4', '5', ' ', '0', '6', '4', '0']\n",
      "['2', '0', '0', '6', ' ', '2', '4', '3', '7', ' ', '8', '0', '5', '4', ' ', '1', '6', '0', '0']\n",
      "['8', '1', '4', '2', ' ', '4', '0', '5', '5', ' ', '1', '7', '7', '6', ' ', '0', '0', '2', '6']\n",
      "['3', '0', '2', '6', ' ', '7', '3', '8', '0', ' ', '1', '2', '4', '1', ' ', '1', '0', '8', '4']\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf=pd.read_csv('credit_card.dat',header=None)\\nfor row in df:\\n  a=str(row)\\n  print(type(a))\\n  #row=row[:-4]\\n  #p=[a[i:i+n] for i in range(0, len(a), 6)]\\n  #print(p)\\n\\n\\ndf\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('credit_card.dat','r') as data:\n",
    "    for row in data:\n",
    "      #print(row)\n",
    "      row=row[:-5]\n",
    "      p=[row[i:i+6]for i in range(0,len(row),6)]\n",
    "      #print(p)\n",
    "      ii=[chr(int(x,2)) for x in p]\n",
    "      print(ii)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "df=pd.read_csv('credit_card.dat',header=None)\n",
    "for row in df:\n",
    "  a=str(row)\n",
    "  print(type(a))\n",
    "  #row=row[:-4]\n",
    "  #p=[a[i:i+n] for i in range(0, len(a), 6)]\n",
    "  #print(p)\n",
    "\n",
    "\n",
    "df\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Load the file \"user_data.json\", filter the data by the \"CreditCardType\" field equals to \"American Express\". Than save the data a to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': '2', 'JobTitle': 'Investment  Advisor', 'EmailAddress': 'Clint_Thorpe5003@bulaffy.com', 'FirstNameLastName': 'Clint Thorpe', 'CreditCard': '7083-8766-0251-2345', 'CreditCardType': 'American Express'}, {'ID': '12', 'JobTitle': 'Retail Trainee', 'EmailAddress': 'Phillip_Carpenter9505@famism.biz', 'FirstNameLastName': 'Phillip Carpenter', 'CreditCard': '3657-0088-0820-5247', 'CreditCardType': 'American Express'}, {'ID': '28', 'JobTitle': 'Project Manager', 'EmailAddress': 'Russel_Graves1378@extex.org', 'FirstNameLastName': 'Russel Graves', 'CreditCard': '6718-4818-8011-6024', 'CreditCardType': 'American Express'}, {'ID': '39', 'JobTitle': 'Stockbroker', 'EmailAddress': 'Leanne_Newton1268@typill.biz', 'FirstNameLastName': 'Leanne Newton', 'CreditCard': '5438-0816-4166-4847', 'CreditCardType': 'American Express'}, {'ID': '57', 'JobTitle': 'Budget Analyst', 'EmailAddress': 'Tony_Giles1960@iatim.tech', 'FirstNameLastName': 'Tony Giles', 'CreditCard': '8130-3425-7573-7745', 'CreditCardType': 'American Express'}, {'ID': '62', 'JobTitle': 'CNC Operator', 'EmailAddress': 'Owen_Allcott5125@bauros.biz', 'FirstNameLastName': 'Owen Allcott', 'CreditCard': '4156-0107-7210-2630', 'CreditCardType': 'American Express'}, {'ID': '68', 'JobTitle': 'Project Manager', 'EmailAddress': 'Liam_Lynn3280@kideod.biz', 'FirstNameLastName': 'Liam Lynn', 'CreditCard': '7152-3247-6053-2233', 'CreditCardType': 'American Express'}, {'ID': '74', 'JobTitle': 'Dentist', 'EmailAddress': 'Regina_Woodcock5820@yahoo.com', 'FirstNameLastName': 'Regina Woodcock', 'CreditCard': '0208-1753-3870-8002', 'CreditCardType': 'American Express'}, {'ID': '81', 'JobTitle': 'HR Specialist', 'EmailAddress': 'Carter_Wallace9614@atink.com', 'FirstNameLastName': 'Carter Wallace', 'CreditCard': '4256-7201-6717-4322', 'CreditCardType': 'American Express'}, {'ID': '92', 'JobTitle': 'Staffing Consultant', 'EmailAddress': 'Maia_Stark2797@jiman.org', 'FirstNameLastName': 'Maia Stark', 'CreditCard': '3851-1403-1734-6321', 'CreditCardType': 'American Express'}, {'ID': '97', 'JobTitle': 'Stockbroker', 'EmailAddress': 'Ciara_Lomax982@bauros.biz', 'FirstNameLastName': 'Ciara Lomax', 'CreditCard': '3702-3440-2472-5424', 'CreditCardType': 'American Express'}, {'ID': '116', 'JobTitle': 'Staffing Consultant', 'EmailAddress': 'Isabel_Ellwood1475@fuliss.net', 'FirstNameLastName': 'Isabel Ellwood', 'CreditCard': '3738-0882-0066-6683', 'CreditCardType': 'American Express'}, {'ID': '148', 'JobTitle': 'CNC Operator', 'EmailAddress': 'Abdul_Townend2202@infotech44.tech', 'FirstNameLastName': 'Abdul Townend', 'CreditCard': '4224-1226-3557-3448', 'CreditCardType': 'American Express'}, {'ID': '150', 'JobTitle': 'Fabricator', 'EmailAddress': 'Caleb_Poulton1735@atink.com', 'FirstNameLastName': 'Caleb Poulton', 'CreditCard': '8203-6875-5225-0341', 'CreditCardType': 'American Express'}, {'ID': '151', 'JobTitle': 'Restaurant Manager', 'EmailAddress': 'Ronald_Lewis6777@deavo.com', 'FirstNameLastName': 'Ronald Lewis', 'CreditCard': '7212-0155-5014-8471', 'CreditCardType': 'American Express'}, {'ID': '154', 'JobTitle': 'Bellman', 'EmailAddress': 'Faith_Seymour3829@twace.org', 'FirstNameLastName': 'Faith Seymour', 'CreditCard': '4170-5186-6887-6558', 'CreditCardType': 'American Express'}, {'ID': '169', 'JobTitle': 'Assistant Buyer', 'EmailAddress': 'Anthony_Hancock9083@qater.org', 'FirstNameLastName': 'Anthony Hancock', 'CreditCard': '0832-3357-6010-6550', 'CreditCardType': 'American Express'}, {'ID': '176', 'JobTitle': 'Healthcare Specialist', 'EmailAddress': 'Isabella_Willson5478@nanoff.biz', 'FirstNameLastName': 'Isabella Willson', 'CreditCard': '5177-4868-4623-0384', 'CreditCardType': 'American Express'}, {'ID': '182', 'JobTitle': 'Pharmacist', 'EmailAddress': 'Stephanie_Darcy3298@bauros.biz', 'FirstNameLastName': 'Stephanie Darcy', 'CreditCard': '0264-4020-5106-5576', 'CreditCardType': 'American Express'}, {'ID': '199', 'JobTitle': 'Investment  Advisor', 'EmailAddress': 'Ryan_Kennedy5565@corti.com', 'FirstNameLastName': 'Ryan Kennedy', 'CreditCard': '3166-6287-6242-7207', 'CreditCardType': 'American Express'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('amex.csv','w') as data:\\n    with open('amex.csv','w') as n:\\n        writer=csv.writer(n)\\n        for row in filtered_dict:\\n            writer.writerow(row.split())\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "data=json.load(open('user_data.json'))\n",
    "# Filter dictionary where name is 'Bob'\n",
    "filtered_dict = [item for item in data if item.get(\"CreditCardType\") == \"American Express\"]\n",
    "print(filtered_dict)\n",
    "df=pd.DataFrame(filtered_dict)\n",
    "df.to_csv('amex.csv',index=False)\n",
    "'''\n",
    "with open('amex.csv','w') as data:\n",
    "    with open('amex.csv','w') as n:\n",
    "        writer=csv.writer(n)\n",
    "        for row in filtered_dict:\n",
    "            writer.writerow(row.split())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Load the file from this url: [https://www.dropbox.com/s/7u3lm737ogbqsg8/mushrooms_categorized.csv?dl=1](https://www.dropbox.com/s/7u3lm737ogbqsg8/mushrooms_categorized.csv?dl=1) with Pandas. \n",
    "+ Explore the data (see the info of the data)\n",
    "+ Draw the histogram of the 'class' field. Decribe wath yuou see\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Load the remote file [https://www.dropbox.com/s/vkl89yce7xjdq4n/regression_generated.csv?dl=1](https://www.dropbox.com/s/vkl89yce7xjdq4n/regression_generated.csv?dl=1) with Pandas and plot a scatter plot all possible combination of the following fields:\n",
    "    \n",
    "  + features_1\n",
    "  + features_2\n",
    "  + features_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Load the same file of point 6, and convert the file to json with Pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
